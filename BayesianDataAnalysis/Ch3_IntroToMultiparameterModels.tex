\documentclass{article}
\usepackage[margin=0.6in]{geometry} 

\usepackage{amsmath,amsthm,amssymb,hyperref}

\usepackage{graphicx}
\usepackage[shortlabels]{enumitem}
\usepackage{tgschola}

\newcommand{\R}{\mathbf{R}}  
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\N}{\mathbf{N}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{claim}[2][Claim]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}

\begin{document}

\large % please keep the text at this size for ease of reading.

% ------------------------------------------ %
%                 START HERE             %
% ------------------------------------------ %

{\Large Page 79 % Replace with appropriate page number 
\hfill  Ch3, Introduction to Multiparameter Models}

\begin{center}
{\Large Xinyu Tan} 
\end{center}
\vspace{0.05in}

% -----------------------------------------------------
% The "enumerate" environment allows for automatic problem numbering.
% To make the number for the next problem, type " \item ". 
% To make sub-problems such as (a), (b), etc., use an "enumerate" within an "enumerate."
% -----------------------------------------------------
 \renewcommand{\labelitemi}{$\textendash$}
\begin{itemize}

% -----------------------------------------------------
% Ex. 1
% -----------------------------------------------------
\item \textbf{1. Binomial and multinomial models}
\begin{enumerate}[(a)]
% a
\item marginal posterior distribution for $\alpha = \frac{\theta_1}{\theta_1 + \theta_2}$.

Suppose that the prior distribution for $\theta = (\theta_1, \theta_2, \cdots, \theta_J) \sim \text{Dirichlet} (a_1, \cdots, a_J)$.  The posterior distribution is
\begin{align*}
\theta_1, \cdots, \theta_J | y &\propto \theta_1^{y_1+a_1-1} \cdots \theta_n^{y_J+a_J-1} \\
&\sim \text{Dirichlet} (y_1+a_1, \cdots, y_J+a_J)
\end{align*}

From the appendix A, the marginal of $(\theta_1, \theta_2, 1-\theta_1-\theta_2) \sim \text{Dirichlet}(y_1+a_1, y_2+a_2,  y_r + a_r)$, where $y_r = y_3 + \cdots+y_J$ and $a_r = a_3 + \cdots + a_J$.

Let $\alpha = h(\theta_1, \theta_2) = \frac{\theta_1}{\theta_1 + \theta_2}$ and $\beta = g(\theta_1, \theta_2) = \theta_1 + \theta_2$, then $\theta_1 = \alpha\beta$ and $\theta_2 = \beta(1-\alpha)$.
\begin{align*}
\alpha, \beta | y &\propto (\alpha\beta)^{a_1+y_1-1} (\beta(1-\alpha))^{a_2+y_2-1} (1-\beta)^{a_r+y_r -1} \begin{vmatrix} \frac{\partial h}{\partial \theta_1} & \frac{\partial h}{\partial \theta_2} \\ \frac{\partial g}{\partial \theta_1}  & \frac{\partial h}{\partial \theta_2} \end{vmatrix}^{-1} \\
&= \alpha^{a_1+y_1-1} (1-\alpha)^{a_2+y_2-1} \beta^{a_1+a_2+y_1+y_2-1} (1-\beta)^{a_r+y_r-1} \\
&\sim \text{Beta} (\alpha | a_1+y_1, a_2+y_2) \text{Beta}(\beta | a_1+a_2+y_1+y_2, a_r+y_r) 
\end{align*}

Since $\alpha$ and $\beta$ belong to separate factors, they are independent: $\alpha \sim \text{Beta}(a_1+y_1, a_2+y_2)$.

\end{enumerate}

% -----------------------------------------------------
% Ex. 2
% -----------------------------------------------------
\item \textbf{2. Comparison of two multinomial observations}

Suppose a uniform prior distribution, then the posterior distribution of $\theta_b, \theta_d, \theta_o$ for pre and post debate are
\begin{align}
(\theta_{b1}, \theta_{d1}, \theta_{o1}) \sim \text{Dirichlet} (294, 307, 38) \\
(\theta_{b2}, \theta_{d2}, \theta_{o2}) \sim \text{Dirichlet} (288, 332, 19)
\end{align}

From Ex1, we know that the posterior distribution for $\theta_b, \theta_b$ follows $\text{Beta}(y_b, y_d)$, then 
\begin{align}
\alpha_1 \sim \text{Beta}(294, 307) \\
\alpha_2 \sim \text{Beta}(288, 332)
\end{align}

The histogram is shown \ref{fig:figure1}.
\begin{figure}[h] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=5in]{{code/ch3/ch3_2.png}} 
   \caption{Histogram for Ex2}
   \label{fig:figure1}
\end{figure}


The posterior probability that there is a shift toward Bush: $p(\alpha_2 > \alpha_1) \Rightarrow p(\alpha_2 - \alpha_1 > 0) = 0.1898$.

% -----------------------------------------------------
% Ex. 3
% -----------------------------------------------------
\item \textbf{3. Estimation from two independent experiments}

\begin{enumerate}[(a)]
\item Since control and experiment groups are independent, we can model them separately. Given a noninformative prior, from Section 3.2, we have
$$
p(\mu|y) \sim t_{n-1}(\bar y, \frac{s^2}{n})
$$

Control group: $\mu_c = 1.013, s_c= 0.24, n_c= 32$, posterior distribution of $\mu_c$
$$
\mu_c | y \sim t_{31}(1.013, 0.24^2/32) 
$$

Treated group: $\mu_t = 1.173, s_t= 0.20, n_t = 36$, posterior distribution of $\mu_t$
$$
\mu_t | y \sim t_{35}(1.173, 0.20^2/36)
$$

\item Posterior distribution for the difference $\mu_t - \mu_c$. Sampling from the independent $t$ distributions, we have the histogram, show in \ref{fig:ex3}:
\begin{figure}[h] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=5in]{{code/ch3/ch3_3.png}} 
   \caption{Histogram for Ex3}
   \label{fig:ex3}
\end{figure}

\end{enumerate}

% -----------------------------------------------------
% Ex. 5
% -----------------------------------------------------
\item \textbf{5. Rounded data}

\begin{enumerate}[(a)]
\item If we pretend that the observations are exact unrounded measurements, with noninformative prior distribution on the mean $\mu$ and variance $\sigma^2$, posterior distribution should follow Eq. 3.2
$$
p(\mu, \sigma^2 | y) \sim \sigma^{-n-2} \exp \left(-\frac{1}{2\sigma^2} [(n-1) s^2 + n (\bar y - \mu)^2] \right)
$$
where $n=5$, $\bar y = 10.4$ and $s^2 = 1.3$.

\item Treat the measurements as rounded. 

Denote $\tilde y$ the rounded, observed measurement, $y$ the exact measurement. Since the observed is rounded to the nearest integer, we have
$$
\tilde y | y \sim I(\tilde y - 0.5 \leq y \leq  \tilde y + 0.5)
$$
where $I(x) = \begin{cases} 1, &x = \text{true} \\ 0, &x = \text{false} \end{cases}$. $y$, unobserved, is sampled from the normal distribution, 
$$
y | \mu, \sigma^2 \sim N(\mu, \sigma^2)
$$

Hence, we have 
$$
\tilde y, y | \mu, \sigma^2 \propto p(\tilde y | y, \mu, \sigma^2) p(y|\mu, \sigma^2) = p(\tilde y | y ) p(y|\mu, \sigma^2) 
$$
Integral over $y$, we have
\begin{align*}
\tilde y | \mu, \sigma^2 &\propto \int p(\tilde y | y ) p(y|\mu, \sigma^2) dy \\
&= \int I(\tilde y - 0.5 \leq y \leq  \tilde y + 0.5)N(y|\mu, \sigma^2) dy \\
&= \int_{\tilde y-0.5}^{\tilde y+0.5} N(y|\mu, \sigma^2) dy \\
&= \Phi (\tilde y + 0.5 | \mu, \sigma^2) - \Phi (\tilde y - 0.5 | \mu, \sigma^2) \\
&= \Phi \left(\frac{\tilde y + 0.5 - \mu}{\sigma} \right) - \Phi \left ( \frac{\tilde y - 0.5 - \mu}{\sigma} \right )
\end{align*}
where $\Phi(x) = \Phi(x | 0, 1)$.

Then, the posterior distribution
\begin{align*}
\mu, \sigma^2 | \tilde y &\propto p(\mu, \sigma^2) p(\tilde y | \mu, \sigma^2) \\
&=  \sigma^{-2} \prod_{i=1}^5 \left ( \Phi \left(\frac{\tilde {y_i} + 0.5 - \mu}{\sigma} \right) - \Phi \left ( \frac{\tilde {y_i} - 0.5 - \mu}{\sigma} \right ) \right )
\end{align*}

\item The difference between incorrect (a) and correct (b) posterior distributions.  
The contour plot of posterior probability at a grid $(\mu, \log \sigma) \in [8, 12] \times [-1, 1]$ is shown in figure \ref{fig:ex5}

\begin{figure}[h] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=7in]{{code/ch3/ch3_5.png}} 
   \caption{Contour plot for Ex5.c}
   \label{fig:ex5}
\end{figure}

The summary statistics are:

Incorrect posterior $\mu$ - mean:10.4         variance:0.214

Incorrect posterior $\sigma$ - mean: 1.2          variance: 0.128

Correct posterior $\mu$ - mean: 10.4          variance:0.175

Correct posterior $\sigma$ - mean: 0.96   variance:0.112

\end{enumerate}


% -----------------------------------------------------
% Ex. 6
% -----------------------------------------------------
\item \textbf{6. Binomial with unknown probability and sample size}

\begin{enumerate}[(a)]
  \item Assume the noninformative prior distribution is $p(\lambda, \theta) \propto \lambda^{-1}$, where $\lambda = \mu \theta$. Therefore, we have $p(\mu, \theta) = \theta \cdot \frac{1}{\mu \theta} = \mu^{-1}$.
  This prior is improper. (exponential integral: \url{https://en.wikipedia.org/wiki/List_of_integrals_of_exponential_functions})
   \begin{align*}
   p(N, \theta)  &= \int_{0}^{\infty} p(N, \mu, \theta) d\mu \\
   	&= \int p(N|\mu, \theta) p(\mu, \theta) d\mu \\
	&= \int \frac{1}{N!} \mu^{N} e^{-\mu} \mu^{-1} d\mu = \frac{(N-1)!}{N!} = \frac{1}{N}
   \end{align*}
 
 \item $y \sim \text{Bin}(N, \theta)$, the likelihood 
  $$
  p(y) = \binom{N}{y} \theta^y (1-\theta)^{N-y} 
  $$
  
  The posterior distribution for $N$ and $\theta$:
  \begin{align*}
  	p(N, \theta|Y) \propto \frac{1}{N} \left (\prod_{i=0}^n  \binom{N}{y_i} \theta^y_i (1-\theta)^{N-y_i} \right ) I(N \geq \max_{i=1}^n y_i)
  \end{align*}
where $Y = (53, 57, 66, 67, 72)$
The contour plot and the scatterplot of the posterior simulations are in figure \ref{fig:ex6}. By simulation, the posterior probability that $N>100$ is $0.952$.

\begin{figure}[h] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=5in]{{code/ch3/ch3_6.png}} 
   \caption{Contour plot and scatterplot for Ex6.b}
   \label{fig:ex6}
\end{figure}
\end{enumerate}

% -----------------------------------------------------
% Ex. 9
% -----------------------------------------------------

\item \textbf{9. Conjugate normal model}

Suppose  $y$ is an i.i.d sample of size $n$ from the distribution $N(\mu, \sigma^2)$. Prior for $(\mu, \sigma^2)$ is
$$
\text{N-Inv-} {\chi}^2 (\mu, \sigma^2 | \mu_0, \sigma_0^2/\kappa_0; \nu_0, \sigma_0^2)
$$

This is derived from 

\begin{align*}
\mu | \sigma^2 &\sim N (\mu_0, \sigma^2/\kappa_0) \\
\sigma^2 &\sim \text{Inv-} \chi^2 (\nu_0, \sigma_0^2)
\end{align*}

Notice the formula for $\text{Inv-}\chi^2 (\nu, s^2)$ is $$p(\theta) \propto s^{\nu} \theta^{-(\nu/2 + 1)} e^{1/(2\theta)}$$ and $\text{N-Inv-} {\chi}^2 (\mu, \sigma^2 | \mu_0, \sigma_0^2/\kappa_0; \nu_0, \sigma_0^2)$ $$p(\mu, \sigma^2) \propto \sigma^{-1} (\sigma^2)^{-(\nu_0/2 + 1)} \exp \left (-\frac{1}{2\sigma^2} [\nu_0 \sigma_0^2 + \kappa_0 (\mu_0 - \mu)^2] \right)$$

Therefore, the posterior 
\begin{align*}
p(\mu, \sigma^2 | y) &\propto  p(\mu, \sigma^2) \prod_{i=1}^n p(y_i | \mu, \sigma^2) \\
	&= \sigma^{-1} (\sigma^2)^{-(\nu_0/2 + 1)} \exp \left (-\frac{1}{2\sigma^2} [\nu_0 \sigma_0^2 + \kappa_0 (\mu_0 - \mu)^2] \right) \times \\
	& \qquad (\sigma^2)^{-n/2} \exp \left ( -\frac{1}{2\sigma^2}[(n-1)s^2 + n (\bar y - \mu)^2] \right )
\end{align*}

\textit{The rest arithmetic is rather dull, but I guess it's good to do it at least once.}

First, merge the terms outside the $\exp(\cdots)$:
$$
\sigma^{-1} \sigma^{-\left( (\nu_0 + n)/ 2 + 1 \right)}
$$

Let's focus on the terms inside $\exp (\cdots)$, discarding the exponential:
\begin{align*}
&\quad -\frac{1}{2\sigma^2} [\nu_0 \sigma_0^2 + \kappa_0 (\mu_0 - \mu)^2] -\frac{1}{2\sigma^2}[(n-1)s^2 + n (\bar y - \mu)^2] \\
&= -\frac{1}{2\sigma^2} \left ( \nu_0\sigma_0^2 + (n-1)s^2 + \kappa_0 (\mu_0 - \mu)^2 + n (\bar y - \mu)^2 \right) \\
&= -\frac{1}{2\sigma^2} \left ( \nu_0\sigma_0^2 + (n-1)s^2  + \kappa_0 \mu_0^2 + n {\bar y}^2 + (\kappa_0 + n) \mu^2 - 2(\kappa_0 \mu_0 + n \bar y) \mu \right ) \\
&= -\frac{1}{2\sigma^2} \left ( \nu_0\sigma_0^2 + (n-1)s^2  + \kappa_0 \mu_0^2 + n {\bar y}^2 + (\kappa_0 + n) \left( \mu - \frac{\kappa_0 \mu_0 + n \bar y}{\kappa_0 + n} \right) ^2  - \frac{(\kappa_0 \mu_0 + n \bar y)^2}{\kappa_0 + n}\right ) \\
&= -\frac{1}{2\sigma^2} \left ( \nu_0\sigma_0^2 + (n-1)s^2  + \frac{\kappa_0 n}{\kappa_0 + n} (\bar y - \mu_0)^2  +  (\kappa_0 + n) \left( \mu - \frac{\kappa_0 \mu_0 + n \bar y}{\kappa_0 + n} \right) ^2 \right)
\end{align*}

Let 
\begin{align*}
\mu_n &= \frac{\kappa_0}{\kappa_0 + n} \mu_0 + \frac{n}{\kappa_0 + n} \bar y  \\
\kappa_n &= \kappa_0 + n \\
\nu_n &= \nu_0 + n \\
\nu_n \sigma_n^2 &= \nu_0\sigma_0^2 + (n-1)s^2  + \frac{\kappa_0 n}{\kappa_0 + n} (\bar y - \mu_0)^2
\end{align*}
, then
$$
p(\mu, \sigma^2 | y) = \text{N-Inv-} \chi^2 (\mu, \sigma^2 | \mu_n, \sigma_n^2 / \kappa_n; \nu_n, \sigma_n^2)
$$



% -----------------------------------------------------
% End
% -----------------------------------------------------
\end{itemize}
\end{document}
