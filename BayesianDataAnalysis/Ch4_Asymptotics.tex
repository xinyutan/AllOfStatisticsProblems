\documentclass{article}
\usepackage[margin=0.6in]{geometry} 

\usepackage{amsmath,amsthm,amssymb,hyperref}

\usepackage{graphicx}
\usepackage[shortlabels]{enumitem}
\usepackage{tgschola}

\newcommand{\R}{\mathbf{R}}  
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\N}{\mathbf{N}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{claim}[2][Claim]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}

\begin{document}

\large % please keep the text at this size for ease of reading.

% ------------------------------------------ %
%                 START HERE             %
% ------------------------------------------ %

{\Large Page 98 % Replace with appropriate page number 
\hfill  Ch4, Asymptotics and Connections to Non-Bayesian Approaches}

\begin{center}
{\Large Xinyu Tan} 
\end{center}
\vspace{0.05in}

% -----------------------------------------------------
% The "enumerate" environment allows for automatic problem numbering.
% To make the number for the next problem, type " \item ". 
% To make sub-problems such as (a), (b), etc., use an "enumerate" within an "enumerate."
% -----------------------------------------------------
 \renewcommand{\labelitemi}{$\textendash$}
\begin{itemize}

% -----------------------------------------------------
% Ex. 1
% -----------------------------------------------------
\item \textbf{1. Normal Approximations}
  \begin{enumerate}[(a)]
    \item Log posterior density:
      \begin{align*}
      \log p(\theta|y_1, \cdots, y_5) &= \log p(\theta) \log (y1, \cdots, y_5 | \theta) \\
      &= \sum_{i=1}^5 \log p(y_i | \theta) \propto \sum_{i=1}^5 \log {\frac{1}{1 + (y_i - \theta)^2}} \\
      &= -\sum_{i=1}^5 \log \left(1 + (y_i - \theta)^2 \right) 
      \end{align*}
      
      Hence, first derivative:
      $$
      \frac{d p(\theta|y)}{d\theta} = 2 \sum_{i=1}^5 \frac{y_i - \theta}{1+(y_i-\theta)^2}
      $$
      Second derivative:
      $$
      \frac{d^2 p(\theta|y)}{d\theta^2} = 2 \sum_{i=1}^5 \frac{(y_i-\theta)^2 - 1}{(y_i - \theta)^2 + 1}
      $$
    
     \item The posterior mode $ \hat \theta = -0.125$
     
     \item The posterior normal approximation:
      \begin{align*}
       \log p(\theta|y) &\approx p(\hat \theta | y) + \frac{1}{2} (\theta - \hat \theta)^2 \times \left[ \frac{d^2p(\theta|y)}{d\theta^2} \right]_{\theta = \hat \theta} \\
       &= -5.45 + \frac{1}{2} \times 1.30 \times (\theta + 0.125)^2 
     \end{align*}
     Therefore, the approximated posterior distribution is $\theta | y \sim N(-0.125, 0.877^2)$. 
    \end{enumerate}

% -----------------------------------------------------
% Ex. 2
% -----------------------------------------------------
\item \textbf{2. Normal Approximation}

 Note: \textit{Trivial arithmetic}

In bioassay example, we have the posterior 
$$
p(\alpha, \beta | y) = \prod_{i=1}^4 \text{logit}^{-1} (\alpha + \beta x_i)^{y_i} \left(1 - \text{logit}^{-1}(\alpha + \beta x_i) \right)^{n_i-y_i}
$$

Denote $\text{logit}^{-1}(x)$ to be $f(x)$; hence, $f(x) = 1/\left(1 + e^{-x}\right)$, the derivative $df(x)/dx = (1 - f) f$. Using $f(x)$, the log likelihood:
$$
\log p(\alpha, \beta | y) = \sum_{i=1}^4 y_i \log f(\alpha + \beta x_i) + (n_i - y_i) \log \left (1 - f(\alpha + \beta x_i) \right)
$$

First compute mode:
\begin{align*}
  \frac{\partial \log p(\alpha, \beta | y)}{\partial \alpha} &= \sum_{i=1}^4 y_i - n_i f(\alpha + \beta x_i) \\
  \frac{\partial \log p(\alpha, \beta | y)}{\partial \beta} &= \sum_{i=1}^4 x_i(y_i - n_i f(\alpha + \beta x_i))
\end{align*}

Second derivative:
\begin{align}
  \frac{\partial^2 \log p(\alpha, \beta | y)}{\partial \alpha^2} &= -\sum_{i=1}^4 n_i f(\alpha + \beta x_i) \left ( 1 - f(\alpha + \beta x_i) \right ) \\
  \frac{\partial^2 \log p(\alpha, \beta | y)}{\partial \alpha \partial \beta} &= - \sum_{i=1}^4 n_i x_i f(\alpha + \beta x_i) ( 1 - f(\alpha + \beta x_i)) \\
  \frac{\partial^2 \log p(\alpha, \beta | y)}{\partial \alpha \partial \beta} &= - \sum_{i=1}^4 n_i x_i^2 f(\alpha + \beta x_i) (1 - f(\alpha + \beta x_i))
\end{align}

The information matrix
$$
I(\alpha, \beta) = \begin{bmatrix} \sum_{i=1}^4 n_i f(\alpha + \beta x_i) \left ( 1 - f(\alpha + \beta x_i) \right) && \sum_{i=1}^4 n_i x_i f(\alpha + \beta x_i) ( 1 - f(\alpha + \beta x_i)) \\ 
\sum_{i=1}^4 n_i x_i f(\alpha + \beta x_i) ( 1 - f(\alpha + \beta x_i)) &&  \sum_{i=1}^4 n_i x_i^2 f(\alpha + \beta x_i) (1 - f(\alpha + \beta x_i)) \end{bmatrix}
$$

% -----------------------------------------------------
% Ex. 9
% -----------------------------------------------------
\item \textbf{9. Point estimation}

  \textit{I did not figure this problem out. I looked at the answer key. And then the following is my understanding from the answer. This is a really nice problem - linking many concepts together.}
\\
\\
Suppose the measurement is $\hat y$. The likelihood
$$
\phi (\theta) = p(\hat y|\theta) = \frac{1}{\sqrt{2 \pi} \sigma} \text{exp} \left(-\frac{1}{2\sigma^2} (\hat y - \theta)^2 \right)
$$

\fbox {\begin{minipage}{40em}
\textit{here is a part I did not think of. So basically, it should be assumed that when $\theta<0$, with $\theta \in [0, 1]$, then $p(\theta < 0)$ should be assigned to $p(\theta = 0)$.}
\end{minipage}
}

Then, 
\begin{align*}
\phi (\theta = 0) &= \int_{-\infty}^0\frac{1}{\sqrt{2 \pi} \sigma} \text{exp} \left(-\frac{1}{2\sigma^2} (\hat y - \theta)^2 \right) d\theta \\
&= \frac{1}{2} \left (1 + \text{erf} \left( \frac{-\hat y}{\sigma \sqrt 2}\right) \right)
\end{align*}
and 
\begin{align*}
\phi (\theta = 1) &= \int_1^{\infty}\frac{1}{\sqrt{2 \pi} \sigma} \text{exp} \left(-\frac{1}{2\sigma^2} (\hat y - \theta)^2 \right) d\theta \\
&= 1 - \frac{1}{2} \left (1 + \text{erf} \left( \frac{1 -\hat y}{\sigma \sqrt 2}\right) \right)
\end{align*}

$\text{erf}(0) = 0$, then when $\sigma \rightarrow \infty$, $\phi(0) \rightarrow 1/2$ and $\phi(1) \rightarrow 1/2$.

Therefore, in the case of max likelihood, we have $\theta$'s estimate to be either 1 or 0: 
$$
\text{MSE}_1 = 0.5 \times \theta^2 + 0.5 \times (1 - \theta)^2 = \frac{1}{2}(1 - 2 \theta + 2 \theta^2)
$$

Posterior distribution:
$$
\psi(\theta) = p(\theta| \hat y) \propto I(0\leq \theta \leq 1) \frac{1}{\sqrt{2 \pi} \sigma} \text{exp} \left(-\frac{1}{2\sigma^2} (\hat y - \theta)^2 \right)
$$

The posterior mean 
\begin{align*}
  \bar \theta &= \frac{\int_0^1 \theta \psi(\theta) d\theta}{\int_0^1 \psi(\theta) d\theta} \\
   &\approx \int_0^1 \theta d\theta \quad \quad (\sigma \rightarrow \infty) \\ &= \frac{1}{2}
\end{align*}

Hence, 
$$\text{MLE}_2 = (0.5 - \theta)^2 = \frac{1}{2}\left( \frac{1}{2} - 2 \theta + 2 \theta^2 \right) < \text{MLE}_1
$$
where $\theta \in [0,1]$

% -----------------------------------------------------
% Ex. 10
% -----------------------------------------------------

% -----------------------------------------------------
% End
% -----------------------------------------------------
\end{itemize}
\end{document}
